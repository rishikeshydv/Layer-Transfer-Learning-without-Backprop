{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rrvt3NUnAYTH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, Model\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras as K\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owOXRt4QAYTI",
    "outputId": "9785ce49-747f-4ca2-a461-69ed11c0918e"
   },
   "outputs": [],
   "source": [
    "#defining training and testing datasets\n",
    "(x_train, y_train), (x_test,y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              Title  \\\n",
      "0           0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
      "1           1                    Crispy Salt and Pepper Potatoes   \n",
      "2           2                        Thanksgiving Mac and Cheese   \n",
      "3           3                 Italian Sausage and Bread Stuffing   \n",
      "4           4                                       Newton's Law   \n",
      "\n",
      "                                         Ingredients  \\\n",
      "0  ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...   \n",
      "1  ['2 large egg whites', '1 pound new potatoes (...   \n",
      "2  ['1 cup evaporated milk', '1 cup whole milk', ...   \n",
      "3  ['1 (¾- to 1-pound) round Italian loaf, cut in...   \n",
      "4  ['1 teaspoon dark brown sugar', '1 teaspoon ho...   \n",
      "\n",
      "                                        Instructions  \\\n",
      "0  Pat chicken dry with paper towels, season all ...   \n",
      "1  Preheat oven to 400°F and line a rimmed baking...   \n",
      "2  Place a rack in middle of oven; preheat to 400...   \n",
      "3  Preheat oven to 350°F with rack in middle. Gen...   \n",
      "4  Stir together brown sugar and hot water in a c...   \n",
      "\n",
      "                                          Image_Name  \\\n",
      "0  miso-butter-roast-chicken-acorn-squash-panzanella   \n",
      "1         crispy-salt-and-pepper-potatoes-dan-kluger   \n",
      "2         thanksgiving-mac-and-cheese-erick-williams   \n",
      "3          italian-sausage-and-bread-stuffing-240559   \n",
      "4                 newtons-law-apple-bourbon-cocktail   \n",
      "\n",
      "                                 Cleaned_Ingredients  \n",
      "0  ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
      "1  ['2 large egg whites', '1 pound new potatoes (...  \n",
      "2  ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
      "3  ['1 (¾- to 1-pound) round Italian loaf, cut in...  \n",
      "4  ['1 teaspoon dark brown sugar', '1 teaspoon ho...  \n"
     ]
    }
   ],
   "source": [
    "#looking into data and its columns\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file_path.csv' with the actual path to your CSV file\n",
    "file_path = 'datasets/source/kaggle_food.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Title', 'Ingredients', 'Instructions', 'Image_Name',\n",
      "       'Cleaned_Ingredients'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will only be using 'Title' and 'Ingredients' for our purpose\n",
    "#so we will be dropping the remaining columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'Ingredients', 'Image_Name',\n",
    "       'Cleaned_Ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Title', 'Instructions'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'text' is your input data and 'label' is your target variable\n",
    "X = df['Instructions'].values    #data\n",
    "y = df['Title'].values           #labels \n",
    "\n",
    "# Convert labels to numerical format using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Embedding layer: Converts words into dense vectors of fixed size\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[43mvocab_size\u001b[49m, output_dim\u001b[38;5;241m=\u001b[39membedding_dim, input_length\u001b[38;5;241m=\u001b[39mmax_sequence_length))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convolutional layer with max pooling\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer: Converts words into dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "\n",
    "# Convolutional layer with max pooling\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Fully connected layers for classification\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.3257 - accuracy: 0.9189 - val_loss: 0.1766 - val_accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "# Train the model and display the activations after each epoch\n",
    "start_time = time.time()\n",
    "history = model.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.1766 - accuracy: 0.9517 - 1s/epoch - 5ms/step\n",
      "Test accuracy: 0.95169997215271\n",
      "Time elapsed:  12.85428500175476\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Time elapsed: ', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install emnist\n",
    "# import emnist\n",
    "# from emnist import extract_training_samples\n",
    "# from emnist import extract_test_samples\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "(test_img, test_labels), (_, _) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emnist.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using EMNIST (Extended MNIST) dataset\n",
    "# target_img,target_labels = extract_training_samples('digits')\n",
    "# #test_img,test_labels = extract_test_samples('letters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the datasets\n",
    "test_img = test_img.reshape((60000, 28, 28, 1))\n",
    "test_img = test_img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate dummy labels\n",
    "def generate_dummy_labels(num_samples, num_classes):\n",
    "    return np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Assuming num_classes in EMNIST is 26 for letters\n",
    "num_classes_emnist = 10\n",
    "\n",
    "# Generate dummy labels for EMNIST\n",
    "dummy_labels = generate_dummy_labels(test_img.shape[0], num_classes_emnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 9s - loss: 6.1756 - accuracy: 0.1001 - 9s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "#testing with no labels\n",
    "test_loss, test_acc = model.evaluate(test_img, dummy_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the accuracy is pretty poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 9s - loss: 6.1824 - accuracy: 0.0728 - 9s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "#testing with labels\n",
    "test_loss, test_acc = model.evaluate(test_img, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the accuracy is still not great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted label: 3\n"
     ]
    }
   ],
   "source": [
    "#predicting the label of a number based image\n",
    "# Load and preprocess the input image\n",
    "img_path = 'num1.png'\n",
    "img = image.load_img(img_path, target_size=(28, 28), color_mode='grayscale')\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x /= 255.0\n",
    "\n",
    "# Predict the label\n",
    "predictions = model.predict(x)\n",
    "predicted_label = np.argmax(predictions[0])\n",
    "\n",
    "print(f'Predicted label: {predicted_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a DANN now and examine the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feature Extractor\n",
    "def build_feature_extractor(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape,name='conv2d_1'),\n",
    "        layers.Conv2D(32, (3,3), activation='relu',name='conv2d_2'),\n",
    "        layers.Conv2D(32, (3,3), activation='relu',name='conv2d_3'),\n",
    "        layers.Flatten(name='flatten')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Label Predictor\n",
    "def build_label_predictor(num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(num_classes, activation='softmax',name='output')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Domain Predictor\n",
    "def build_domain_predictor():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu',name='dense_1'),\n",
    "        layers.Dense(1, activation='sigmoid',name='dense_2')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete DANN model\n",
    "def build_dann(input_shape, num_classes):\n",
    "    feature_extractor = build_feature_extractor(input_shape)\n",
    "    label_predictor = build_label_predictor(num_classes)\n",
    "    domain_predictor = build_domain_predictor()\n",
    "\n",
    "    # Define inputs\n",
    "    input_data = layers.Input(shape=input_shape)\n",
    "    label = layers.Input(shape=(num_classes,))\n",
    "    domain_label = layers.Input(shape=(1,))\n",
    "    \n",
    "    # Feature extractor output\n",
    "    feature_output = feature_extractor(input_data)\n",
    "    \n",
    "    # Label prediction branch\n",
    "    label_output = label_predictor(feature_output)\n",
    "    \n",
    "    # Domain prediction branch\n",
    "    domain_output = domain_predictor(feature_output)\n",
    "    \n",
    "    dann_model = models.Model(inputs=[input_data, label, domain_label], \n",
    "                              outputs=[label_output, domain_output])\n",
    "    \n",
    "    return dann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "def label_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "def domain_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the DANN model\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10\n",
    "\n",
    "dann_model = build_dann(input_shape, num_classes)\n",
    "\n",
    "dann_model.compile(optimizer='Adam',\n",
    "                   loss=[label_loss, domain_loss],\n",
    "                   loss_weights=[1.0,0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare domain labels\n",
    "domain_label_source = np.zeros((x_train.shape[0], 1))  # Source domain label is 0\n",
    "domain_label_target = np.ones((test_img.shape[0], 1))   # Target domain label is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the train labels\n",
    "#shape of y_train is (6000,) initially despite having 10 classes\n",
    "# it means that it contains class labels as integers rather than being one-hot encoded.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate source and target data\n",
    "x_combined = np.vstack((x_train, test_img))\n",
    "y_combined = np.vstack((y_train, np.zeros((y_train.shape))))  # Labels for target domain can be ignored\n",
    "domain_labels_combined = np.vstack((domain_label_source, domain_label_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/var/folders/b8/466710_d78q31z8wgrvw2cc00000gn/T/ipykernel_84611/2340007477.py\", line 3, in label_loss  *\n        return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 28, 28, 1) and (32, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[448], line 5\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the DANN\u001b[39;00m\n",
      "\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;32m----> 5\u001b[0m \u001b[43mdann_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdomain_labels_combined\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      9\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m/var/folders/b8/466710_d78q31z8wgrvw2cc00000gn/T/__autograph_generated_file_kzzdgc2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/var/folders/b8/466710_d78q31z8wgrvw2cc00000gn/T/__autograph_generated_file0jhirp2t.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__label_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mcategorical_crossentropy, (ag__\u001b[39m.\u001b[39mld(y_true), ag__\u001b[39m.\u001b[39mld(y_pred)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n",
      "\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/var/folders/b8/466710_d78q31z8wgrvw2cc00000gn/T/ipykernel_84611/2340007477.py\", line 3, in label_loss  *\n",
      "        return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n",
      "        return backend.categorical_crossentropy(\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n",
      "        target.shape.assert_is_compatible_with(output.shape)\n",
      "\n",
      "    ValueError: Shapes (32, 28, 28, 1) and (32, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Train the DANN\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dann_model.fit([x_combined,y_combined,domain_labels_combined], \n",
    "               [x_combined,y_combined], \n",
    "               epochs=1)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
