{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be building a DANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, Model\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras as K\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              Title  \\\n",
      "0           0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
      "1           1                    Crispy Salt and Pepper Potatoes   \n",
      "2           2                        Thanksgiving Mac and Cheese   \n",
      "3           3                 Italian Sausage and Bread Stuffing   \n",
      "4           4                                       Newton's Law   \n",
      "\n",
      "                                         Ingredients  \\\n",
      "0  ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...   \n",
      "1  ['2 large egg whites', '1 pound new potatoes (...   \n",
      "2  ['1 cup evaporated milk', '1 cup whole milk', ...   \n",
      "3  ['1 (¾- to 1-pound) round Italian loaf, cut in...   \n",
      "4  ['1 teaspoon dark brown sugar', '1 teaspoon ho...   \n",
      "\n",
      "                                        Instructions  \\\n",
      "0  Pat chicken dry with paper towels, season all ...   \n",
      "1  Preheat oven to 400°F and line a rimmed baking...   \n",
      "2  Place a rack in middle of oven; preheat to 400...   \n",
      "3  Preheat oven to 350°F with rack in middle. Gen...   \n",
      "4  Stir together brown sugar and hot water in a c...   \n",
      "\n",
      "                                          Image_Name  \\\n",
      "0  miso-butter-roast-chicken-acorn-squash-panzanella   \n",
      "1         crispy-salt-and-pepper-potatoes-dan-kluger   \n",
      "2         thanksgiving-mac-and-cheese-erick-williams   \n",
      "3          italian-sausage-and-bread-stuffing-240559   \n",
      "4                 newtons-law-apple-bourbon-cocktail   \n",
      "\n",
      "                                 Cleaned_Ingredients  \n",
      "0  ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
      "1  ['2 large egg whites', '1 pound new potatoes (...  \n",
      "2  ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
      "3  ['1 (¾- to 1-pound) round Italian loaf, cut in...  \n",
      "4  ['1 teaspoon dark brown sugar', '1 teaspoon ho...  \n"
     ]
    }
   ],
   "source": [
    "#looking into data and its columns\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file_path.csv' with the actual path to your CSV file\n",
    "file_path = 'datasets/source/kaggle_food.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will only be using 'Title' and 'Ingredients' for our purpose\n",
    "#so we will be dropping the remaining columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'Ingredients', 'Image_Name',\n",
    "       'Cleaned_Ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are creating a fake labels for temporary test\n",
    "#1/3 part of the labels to 'appetizers'. another 1/3 part to 'dinner' and the last 1/3 part to 'desserts'\n",
    "\n",
    "total_size = len(df)\n",
    "category_size = total_size // 3\n",
    "\n",
    "df.loc[:category_size - 1, 'Title'] = 'Appetizers'\n",
    "df.loc[category_size:2*category_size - 1, 'Title'] = 'Dinner'\n",
    "df.loc[2*category_size:total_size - 1, 'Title'] = 'Desserts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the dataframe\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Assuming df['Instructions'] is your text data\n",
    "df['Instructions'].fillna('', inplace=True)  # Replace NaN values with an empty string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#defining the parameters\n",
    "num_classes = 1\n",
    "embedding_dim = 100\n",
    "\n",
    "# Assuming df['Instructions'] is your text data, we  tokenize the input dataset into tokens for the CNN model\n",
    "#Tokenizer Initialization and Fitting:\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Instructions'])\n",
    "\n",
    "#Vocabulary Size and Maximum Sequence Length Calculation:\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_sequence_length = max(df['Instructions'].apply(lambda x: len(x.split())))\n",
    "\n",
    "#Texts to Sequences:\n",
    "sequences = tokenizer.texts_to_sequences(df['Instructions'])\n",
    "\n",
    "#Padding Sequences\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'text' is your input data and 'label' is your target variable\n",
    "X = data   #data\n",
    "y = df['Title'].values           #labels \n",
    "\n",
    "# Convert labels to numerical format using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feature Extractor\n",
    "def build_feature_extractor(vocab_size,embedding_dim,max_sequence_length):\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "        layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        layers.GlobalMaxPooling1D()\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Label Predictor\n",
    "def build_label_predictor(num_classes):\n",
    "    model = models.Sequential([\n",
    "        Dense(units=num_classes, activation='softmax',name='output')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Domain Predictor\n",
    "def build_domain_predictor():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64,activation='relu',name='dense_1'),\n",
    "        layers.Dense(1,activation='sigmoid',name='dense_2')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete DANN model\n",
    "def build_dann(vocab_size,embedding_dim,max_sequence_length, num_classes):\n",
    "    feature_extractor = build_feature_extractor(vocab_size,embedding_dim,max_sequence_length)\n",
    "    label_predictor = build_label_predictor(num_classes)\n",
    "    domain_predictor = build_domain_predictor()\n",
    "\n",
    "    # Define inputs\n",
    "    input_data = layers.Input(shape=(max_sequence_length,))\n",
    "    label = layers.Input(shape=(num_classes,))\n",
    "    domain_label = layers.Input(shape=(1,))\n",
    "    \n",
    "    # Feature extractor output\n",
    "    feature_output = feature_extractor(input_data)\n",
    "    \n",
    "    # Label prediction branch\n",
    "    label_output = label_predictor(feature_output)\n",
    "    \n",
    "    # Domain prediction branch\n",
    "    domain_output = domain_predictor(feature_output)\n",
    "    \n",
    "    dann_model = models.Model(inputs=[input_data, label, domain_label], \n",
    "                              outputs=[label_output, domain_output])\n",
    "    \n",
    "    return dann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "def label_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "def domain_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_5\" (type Sequential).\n\nInput 0 of layer \"global_max_pooling1d_5\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 28, 24, 128)\n\nCall arguments received by layer \"sequential_5\" (type Sequential):\n  • inputs=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 5\u001b[0m dann_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dann\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m dann_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                    loss\u001b[38;5;241m=\u001b[39m[label_loss, domain_loss],\n\u001b[1;32m      9\u001b[0m                    loss_weights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m0.1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m, in \u001b[0;36mbuild_dann\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m domain_label \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Feature extractor output\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m feature_output \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Label prediction branch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m label_output \u001b[38;5;241m=\u001b[39m label_predictor(feature_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[0;32m--> 235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_5\" (type Sequential).\n\nInput 0 of layer \"global_max_pooling1d_5\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 28, 24, 128)\n\nCall arguments received by layer \"sequential_5\" (type Sequential):\n  • inputs=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Build and compile the DANN model\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10\n",
    "\n",
    "dann_model = build_dann(input_shape, num_classes)\n",
    "\n",
    "dann_model.compile(optimizer='Adam',\n",
    "                   loss=[label_loss, domain_loss],\n",
    "                   loss_weights=[1.0,0.1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
